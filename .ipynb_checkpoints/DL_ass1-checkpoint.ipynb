{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fed4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "#importing the dataset from keras library\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# #one data image per label\n",
    "# fig,ax=plt.subplots(nrows=10,figsize=(15,15))\n",
    "# for i in range(10):\n",
    "#     ax[i].set_title(\"\\n class {} image\".format(i))\n",
    "#     ax[i].axis(\"off\")\n",
    "#     x=x_train[y_train==i]\n",
    "#     ax[i].imshow(x[0,:,:],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8364d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the data between 0-1\n",
    "x_train=x_train/255\n",
    "x_train=x_train.astype(float);\n",
    "x_test=x_test/255\n",
    "x_test=x_test.astype(float);\n",
    "\n",
    "#flattening the data points to 1D\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self,epochs,noOfHL,NeuronsPL,noOfClass,X_train,y_train,x_test,y_test,learningRate):\n",
    "        self.noOfHL=noOfHL\n",
    "        self.ListOfNeuronsPL=NeuronsPL\n",
    "        self.noOfClass=noOfClass\n",
    "        self.x_train=x_train\n",
    "        self.epochs=epochs\n",
    "        self.learningRate=learningRate\n",
    "        #initialise the initial weights matrix which contains noOfHiddenLayers+1  weight matrices\n",
    "        self.W=initialize_weights(self)\n",
    "        #initialise the initial biases matrix which contains noOfHiddenLayers+1  biases matrices\n",
    "        self.b=initialize_biases(self)\n",
    "\n",
    "    #returns the weight matrix for the initial configuration, we have used 1 indexing for weights\n",
    "    def initialize_weight(self):\n",
    "        weight=[]\n",
    "        for(i in range(self.noOfHL+1)):\n",
    "            if(i==0)\n",
    "                continue\n",
    "            if(i==1)\n",
    "                w=np.random.normal(0, 1, size=(self.ListOfNeuronsPL[i-1],self.x_train.shape[1]))\n",
    "            else\n",
    "                w=np.random.normal(0,1,size=(self.ListOfNeuronPL[i-1],self.ListOfNeuronPL[i-2]))\n",
    "            weight[i]=w\n",
    "        w=np.random.normal(0,1,size=(self.noOfClass,self.ListOfNeuronPL[self.noOfHL-1]))\n",
    "        weight[self.noOfH+1]=w\n",
    "        return weight\n",
    "    #returns the biases matrix for the initial configurtion, we have used 1 indexing for biases\n",
    "    def initialize_biases(self):\n",
    "        biases=[]\n",
    "        for(i in range(self.noOfHL+1)):\n",
    "            if(i==0)\n",
    "                continue\n",
    "            else\n",
    "                b=np.ones(self.ListOfNeuronsPL[i-1])\n",
    "            biases[i]=b\n",
    "        b=np.ones(self.noOfClass)\n",
    "        biases[self.noOfHL+1]=b\n",
    "        return biases\n",
    "    #returns sigmoid value of a variable x\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    #returns softmax value of a variable x\n",
    "    def softmax(x):\n",
    "        exp_x=np.exp(x)\n",
    "        sum_e=np.sum(exp_x)\n",
    "        return exp_x/sum_e\n",
    "    \n",
    "    def activationFunc(self,A):\n",
    "        H=self.sigmoid(A)\n",
    "        return H\n",
    "    \n",
    "    #calculates and returns the predicted values of y using the output Function\n",
    "    def outputFunc(self,A):\n",
    "        yhat=self.softmax(A)\n",
    "        return yhat\n",
    "        \n",
    "    #calculates all the H,A and yhat in the forward propogation of backpropogation\n",
    "    def forwardPropogation(self,i):\n",
    "        L=self.noOfHL+1\n",
    "        k=self.noOfClass\n",
    "        H=[0]*L\n",
    "        A=[0]*L\n",
    "        H[0]=self.x_train[i]\n",
    "        for i in range(L-1):\n",
    "            A[i+1]=self.b[i+1]+np.dot(self.W[i+1],H[i])\n",
    "            H[i+1]=self.activationFunc(A[i+1])\n",
    "        A[L]=self.b[L]+np.dot(self.W[L],H[L-1])\n",
    "        yhat=outputFunc(A[L])\n",
    "        return  H,A,yhat\n",
    "        \n",
    "    def backwardPropogation():\n",
    "        \n",
    "    def _sgd(self):\n",
    "        epochs=self.epochs\n",
    "        L=self.noOfHL+1\n",
    "        k=self.noOfClass\n",
    "        x_train=self.x_train\n",
    "        y_train=self.y_train\n",
    "        eta=self.learningRate\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(x_train.shape[0]):\n",
    "                Hs,As,yhat=forwardPropogation(self,i)\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
