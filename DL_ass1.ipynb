{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fed4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "#importing the dataset from keras library\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# #one data image per label\n",
    "# fig,ax=plt.subplots(nrows=10,figsize=(15,15))\n",
    "# for i in range(10):\n",
    "#     ax[i].set_title(\"\\n class {} image\".format(i))\n",
    "#     ax[i].axis(\"off\")\n",
    "#     x=x_train[y_train==i]\n",
    "#     ax[i].imshow(x[0,:,:],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8364d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the data between 0-1\n",
    "x_train=x_train/255\n",
    "x_train=x_train.astype(float);\n",
    "x_test=x_test/255\n",
    "x_test=x_test.astype(float);\n",
    "\n",
    "#flattening the data points to 1D\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "385f2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self,epochs,noOfHL,NeuronsPL,noOfClass,X_train,y_train,x_test,y_test,learningRate,batchSize):\n",
    "        self.noOfHL=noOfHL\n",
    "        self.ListOfNeuronsPL=NeuronsPL\n",
    "        self.noOfClass=noOfClass\n",
    "        self.x_train=x_train\n",
    "        self.y_train=y_train\n",
    "        self.epochs=epochs\n",
    "        self.learningRate=learningRate\n",
    "        self.batchSize=batchSize\n",
    "        #initialise the initial weights matrix which contains noOfHiddenLayers+1  weight matrices\n",
    "        self.W=self.initialize_weights()\n",
    "        #initialise the initial biases matrix which contains noOfHiddenLayers+1  biases matrices\n",
    "        self.b=self.initialize_biases()\n",
    "\n",
    "    #returns the weight matrix for the initial configuration, we have used 1 indexing for weights\n",
    "    def initialize_weights(self):\n",
    "        weight=[0]*(self.noOfHL+2)\n",
    "        for i in range(self.noOfHL+1):\n",
    "            if(i==0):\n",
    "                continue\n",
    "            if(i==1):\n",
    "                w=np.random.normal(0, 1, size=(self.ListOfNeuronsPL[i-1],self.x_train.shape[1]))\n",
    "            else:\n",
    "                w=np.random.normal(0,1,size=(self.ListOfNeuronsPL[i-1],self.ListOfNeuronsPL[i-2]))\n",
    "            weight[i]=w\n",
    "        w=np.random.normal(0,1,size=(self.noOfClass,self.ListOfNeuronsPL[self.noOfHL-1]))\n",
    "        weight[self.noOfHL+1]=w\n",
    "        return weight\n",
    "    #returns the biases matrix for the initial configurtion, we have used 1 indexing for biases\n",
    "    def initialize_biases(self):\n",
    "        biases=[0]*(self.noOfHL+2)\n",
    "        for i in range(self.noOfHL+1):\n",
    "            if(i==0):\n",
    "                continue\n",
    "            else:\n",
    "                b=np.ones(self.ListOfNeuronsPL[i-1])\n",
    "            biases[i]=b\n",
    "        b=np.ones(self.noOfClass)\n",
    "        biases[self.noOfHL+1]=b\n",
    "        return biases\n",
    "    #returns sigmoid value of a variable x\n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    #returns softmax value of a variable x\n",
    "    def softmax(self,x):\n",
    "        exp_x=np.exp(x)\n",
    "        sum_e=np.sum(exp_x)\n",
    "        return exp_x/sum_e\n",
    "    \n",
    "    def activationFunc(self,A):\n",
    "        A=np.array(A)\n",
    "        H=self.sigmoid(A)\n",
    "        return H\n",
    "    \n",
    "    #calculates and returns the predicted values of y using the output Function\n",
    "    def outputFunc(self,A):\n",
    "        yhat=self.softmax(A)\n",
    "        return yhat\n",
    "        \n",
    "    #calculates all the H,A and yhat in the forward propogation of backpropogation\n",
    "    def forwardPropogation(self,i):\n",
    "        L=self.noOfHL+1\n",
    "        k=self.noOfClass\n",
    "        H=[0]*(L)\n",
    "        A=[0]*(L+1)\n",
    "        H[0]=np.array(self.x_train[i])\n",
    "        for i in range(L-1):\n",
    "            A[i+1]=self.b[i+1]+np.dot(self.W[i+1],H[i])\n",
    "            H[i+1]=self.activationFunc(A[i+1])\n",
    "        A[L]=self.b[L]+np.dot(self.W[L],H[L-1])\n",
    "        yhat=self.outputFunc(A[L])\n",
    "        yhat=np.array(yhat)\n",
    "        #print(\"yhat:  \\n\",yhat.shape)\n",
    "        return  H,A,yhat\n",
    "    \n",
    "    #derivative of loss wrt to activation of last layer if output function used is softmax\n",
    "    def derivative_wrt_outputFunc(self,yhat,y_train,i):\n",
    "        k=self.noOfClass\n",
    "        e_y=np.zeros(k)\n",
    "        e_y[y_train[i]]=1\n",
    "        return -1*(e_y-yhat)\n",
    "    def cal_activationFun_grad(self,As):\n",
    "#         print(type(As))\n",
    "        g_dash=[]\n",
    "        for i in As:\n",
    "            g_dash.append(self.sigmoid(i)*(1-self.sigmoid(i)))\n",
    "        return g_dash\n",
    "        \n",
    "    def backwardPropogation(self,i,Hs,As,yhat,y_train):\n",
    "        W=self.W\n",
    "        L=self.noOfHL+1\n",
    "        weights_grad=[0]*(L+1)\n",
    "        biases_grad=[0]*(L+1)\n",
    "        activation_grad=[0]*(L+1)\n",
    "        preactivation_grad=[0]*(L+1)\n",
    "        activationFunc_grad=[]\n",
    "        preactivation_grad[L]=self.derivative_wrt_outputFunc(yhat,y_train,i)\n",
    "        for k in range(L+1)[::-1]:\n",
    "            if(i==0):\n",
    "                continue\n",
    "            #gradient of loss wrt to weights at layer k\n",
    "            weights_grad[k]=np.outer(preactivation_grad[k],np.transpose(Hs[k-1]))\n",
    "#             print(\"weights_grad wrt loss at level {} is {}\".format(k,type(weights_grad[k])))\n",
    "            \n",
    "            #gradient of loss wrt to biases at layer k\n",
    "            biases_grad[k]=preactivation_grad[k]\n",
    "#             print(\"baises_grad wrt loss at level {} is {}\".format(k,biases_grad[k].shape))\n",
    "            \n",
    "            #for the next layer calculating gradient of loss wrt to activation\n",
    "            activation_grad[k-1]=np.dot(np.transpose(W[k]),preactivation_grad[k])\n",
    "#             print(\"activation_grad wrt loss at level {} is {}\".format(k,activation_grad[k-1].shape))\n",
    "            \n",
    "            #calculate gradient of activation function wrt preactivation of previous layer\n",
    "            if(k>1):\n",
    "                activationFunc_grad=self.cal_activationFun_grad(As[k-1])\n",
    "            \n",
    "                #for the next layer calculating gradient of loss wrt to preactivation\n",
    "                preactivation_grad[k-1]=np.multiply(activation_grad[k-1],activationFunc_grad)\n",
    "            \n",
    "        return weights_grad,biases_grad\n",
    "    \n",
    "    def acc_grad(self,final_grad,f_g):\n",
    "        L=self.noOfHL+1\n",
    "        for i in range(L+1):\n",
    "            if(i==0):\n",
    "                continue\n",
    "            final_grad[i]=final_grad[i]+f_g[i]\n",
    "        return final_grad\n",
    "    def updateWeights(self,eta,weights_grad):\n",
    "        L=self.noOfHL+1\n",
    "        for i in range(L+1):\n",
    "            if(i==0):\n",
    "                continue\n",
    "            self.W[i]=self.W[i]-(eta)*weights_grad[i]\n",
    "        return\n",
    "    \n",
    "    def updateBiases(self,eta,biases_grad):\n",
    "        L=self.noOfHL+1\n",
    "        for i in range(L+1):\n",
    "            if(i==0):\n",
    "                continue\n",
    "            self.b[i]=self.b[i]-(eta)*biases_grad[i]\n",
    "        return\n",
    "    \n",
    "    def crossEntropy(self,yhat,i):\n",
    "        return -1*np.log(yhat[self.y_train[i]])\n",
    "            \n",
    "        \n",
    "    def _sgd(self):\n",
    "        epochs=self.epochs\n",
    "        L=self.noOfHL+1\n",
    "        k=self.noOfClass\n",
    "        x_train=self.x_train\n",
    "        y_train=self.y_train\n",
    "        eta=self.learningRate\n",
    "        batchSize=self.batchSize\n",
    "        deltaw=[]\n",
    "        deltab=[]\n",
    "        loss=[]\n",
    "        trainingLoss=[]\n",
    "        for epoch in range(epochs):\n",
    "            loss=[]\n",
    "            for i in range(x_train.shape[0]):\n",
    "                if(i%batchSize==0):\n",
    "                    if(i!=0):\n",
    "                        #update the weights and biases\n",
    "                        self.updateWeights(eta,deltaw)\n",
    "                        self.updateBiases(eta,deltab)\n",
    "                    Hs,As,yhat=self.forwardPropogation(i)\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    if(i==0):\n",
    "                        deltaw=w_g\n",
    "                        deltab=b_g\n",
    "                    else:\n",
    "                        deltaw=self.acc_grad(deltaw,w_g)\n",
    "                        deltab=self.acc_grad(deltab,b_g)\n",
    "                else:\n",
    "                    Hs,As,yhat=self.forwardPropogation(i)\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    deltaw=self.acc_grad(deltaw,w_g)\n",
    "                    deltab=self.acc_grad(deltab,b_g)\n",
    "                #append loss for this datapoint\n",
    "                loss.append(self.crossEntropy(yhat,i))   \n",
    "            self.updateWeights(eta,deltaw)\n",
    "            self.updateBiases(eta,deltab)\n",
    "            trainingLoss.append(np.mean(loss))\n",
    "            print(\"The loss after epoch:{} is {}\".format(epoch,trainingLoss[epoch]))\n",
    "            \n",
    "               \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7143e92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after epoch:0 is 1.600958029669534\n",
      "The loss after epoch:1 is 1.1118926688941144\n",
      "The loss after epoch:2 is 1.0192072254654014\n",
      "The loss after epoch:3 is 0.9668913064847112\n",
      "The loss after epoch:4 is 0.9783237165441742\n",
      "The loss after epoch:5 is 0.995941746849247\n",
      "The loss after epoch:6 is 0.912857240367062\n",
      "The loss after epoch:7 is 0.9692652967050965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jatin\\AppData\\Local\\Temp\\ipykernel_3980\\3306305528.py:44: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss after epoch:8 is 1.0130265494291895\n",
      "The loss after epoch:9 is 0.949381156494241\n"
     ]
    }
   ],
   "source": [
    "NeuronsPL=[16,32]\n",
    "FNNN=FeedForwardNN(10,2,NeuronsPL,10,x_train,y_train,x_test,y_test,0.0001,50)\n",
    "FNNN._sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e18fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
