{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "385f2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "class FeedForwardNN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        epochs,\n",
    "        noOfHL,\n",
    "        NeuronsPL,\n",
    "        noOfClass,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        x_valid,\n",
    "        y_valid,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        optimizer,\n",
    "        activationfunction,\n",
    "        learningRate,\n",
    "        batchSize,\n",
    "        initialize,\n",
    "        lossfunction,\n",
    "        gamma=0.9,\n",
    "        Beta=0.5,\n",
    "        Beta1=0.9,\n",
    "        Beta2=0.999,\n",
    "        epsilon=0.0001\n",
    "    ):\n",
    "        self.noOfHL=noOfHL\n",
    "        self.NeuronsPL=NeuronsPL\n",
    "        self.noOfClass=noOfClass\n",
    "        self.x_train=x_train\n",
    "        self.y_train=y_train\n",
    "        self.x_valid=x_valid\n",
    "        self.y_valid=y_valid\n",
    "        self.x_test=x_test\n",
    "        self.y_test=y_test\n",
    "        self.activationfunction=activationfunction\n",
    "        self.epochs=epochs\n",
    "        self.learningRate=learningRate\n",
    "        self.batchSize=batchSize\n",
    "        self.Optimizers={\n",
    "            \"SGD\": self._sgd,\n",
    "            \"MGD\": self._mgd,\n",
    "            \"NAG\": self._nag,\n",
    "            \"RMSPROP\": self._rmsProp,\n",
    "            \"ADAM\": self._adam,\n",
    "            \"NADAM\": self._nadam,\n",
    "        }\n",
    "        self.initialize=initialize\n",
    "        #initialise the initial weights matrix which contains noOfHiddenLayers+1  weight matrices\n",
    "        self.W=self.initialize_weights()\n",
    "        #initialise the initial biases matrix which contains noOfHiddenLayers+1  biases matrices\n",
    "        self.b=self.initialize_biases()\n",
    "        self.lossfunction=lossfunction\n",
    "        self.optimizer=self.Optimizers[optimizer]\n",
    "        self.gamma=gamma\n",
    "        self.beta=Beta\n",
    "        self.Beta1=Beta1\n",
    "        self.Beta2=Beta2\n",
    "        self.epsilon=epsilon\n",
    "\n",
    "    #returns the weight matrix for the initial configuration, we have used 1 indexing for weights\n",
    "    def initialize_weights(self):\n",
    "        weight=[0]*(self.noOfHL+2)\n",
    "        if(initialize==\"RANDOM\"):\n",
    "            for i in range(self.noOfHL+1):\n",
    "                if(i==0):\n",
    "                    continue\n",
    "                if(i==1):\n",
    "                    w=np.random.uniform(-1, 1, size=(self.NeuronsPL,self.x_train.shape[1]))\n",
    "                else:\n",
    "                    w=np.random.uniform(-1,1,size=(self.NeuronsPL,self.NeuronsPL))\n",
    "                weight[i]=w\n",
    "            w=np.random.uniform(-1,1,size=(self.noOfClass,self.NeuronsPL))\n",
    "            weight[self.noOfHL+1]=w\n",
    "\n",
    "        #initialising the weights with xavier\n",
    "        if(initialize==\"XAVIER\"):\n",
    "            for i in range(self.noOfHL+1):\n",
    "                if(i==0):\n",
    "                    continue\n",
    "                if(i==1):\n",
    "                    ni=self.NeuronsPL\n",
    "                    no=self.x_train.shape[1]\n",
    "                    w=np.random.uniform(-(6/(ni+no))**0.5,(6/(ni+no))**0.5,size=(ni,no))\n",
    "                else:\n",
    "                    ni=self.NeuronsPL\n",
    "                    no=self.NeuronsPL\n",
    "                    w=np.random.uniform(-(6/(ni+no))**0.5,(6/(ni+no))**0.5,size=(ni,no))\n",
    "                weight[i]=w\n",
    "            ni=self.noOfClass\n",
    "            no=self.NeuronsPL\n",
    "            w=np.random.uniform(-(6/(ni+no))**0.5,(6/(ni+no))**0.5,size=(ni,no))\n",
    "            weight[self.noOfHL+1]=w\n",
    "        return weight\n",
    "                    \n",
    "    #returns the biases matrix for the initial configurtion, we have used 1 indexing for biases\n",
    "    def initialize_biases(self):\n",
    "        biases=[0]*(self.noOfHL+2)\n",
    "        if(initialize==\"XAVIER\"):\n",
    "            for i in range(self.noOfHL+1):\n",
    "                if(i==0):\n",
    "                    continue\n",
    "                else:\n",
    "                    b=np.random.uniform(-(6/(self.NeuronsPL+1))**0.5,(6/(self.NeuronsPL+1))**0.5,size=(self.NeuronsPL))\n",
    "                biases[i]=b\n",
    "            b=np.random.uniform(-(6/(self.noOfClass+1))**0.5,(6/(self.noOfClass+1))**0.5,size=(self.noOfClass))\n",
    "            biases[self.noOfHL+1]=b\n",
    "\n",
    "        if(initialize==\"NORMAL\"):\n",
    "            for i in range(self.noOfHL+1):\n",
    "                if(i==0):\n",
    "                    continue\n",
    "                else:\n",
    "                    b=np.random.uniform(-1, 1, size=(self.NeuronsPL))\n",
    "                biases[i]=b\n",
    "            b=np.random.uniform(-1, 1, size=(self.noOfClass))\n",
    "            biases[self.noOfHL+1]=b\n",
    "        return biases\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def sin(self,x):\n",
    "        return np.sin(x)\n",
    "    \n",
    "    def relu(self,x):\n",
    "        np.maximum(x,0)\n",
    "        \n",
    "    \n",
    "    #returns softmax value of a variable x\n",
    "    def softmax(self,x):\n",
    "        x_max=np.max(x)\n",
    "        exp_x=np.exp(x-x_max)\n",
    "        sum_e=np.sum(exp_x)\n",
    "        return exp_x/sum_e\n",
    "    \n",
    "    def activationFunc(self,A):\n",
    "        A=np.array(A)\n",
    "        if self.activationfunction==\"SIGMOID\":\n",
    "            return self.sigmoid(A)\n",
    "        if self.activationfunction==\"RELU\":\n",
    "            return self.relu(A)\n",
    "        if self.activationfunction==\"TANH\":\n",
    "            return self.tanh(A)\n",
    "        if self.activationfunction==\"SIN\":\n",
    "            return self.sin(A)\n",
    "    \n",
    "    #calculates and returns the predicted values of y using the output Function\n",
    "    def outputFunc(self,A):\n",
    "        yhat=self.softmax(A)\n",
    "        return yhat\n",
    "        \n",
    "    #calculates all the H,A and yhat in the forward propogation of backpropogation\n",
    "    def forwardPropogation(self,x):\n",
    "        L=self.noOfHL+1\n",
    "        k=self.noOfClass\n",
    "        H=[0]*(L)\n",
    "        A=[0]*(L+1)\n",
    "        H[0]=x\n",
    "        for i in range(L-1):\n",
    "            A[i+1]=self.b[i+1]+np.dot(self.W[i+1],H[i])\n",
    "            H[i+1]=self.activationFunc(A[i+1])\n",
    "        A[L]=self.b[L]+np.dot(self.W[L],H[L-1])\n",
    "        yhat=self.outputFunc(A[L])\n",
    "        yhat=np.array(yhat)\n",
    "        return  H,A,yhat\n",
    "    \n",
    "    #derivative of loss wrt to activation of last layer if output function used is softmax\n",
    "    def derivative_wrt_lossFunc(self,yhat,y_train,i):\n",
    "        k=self.noOfClass\n",
    "        e_l=np.zeros(k)\n",
    "        e_l[y_train[i]]=1\n",
    "        if self.lossfunction==\"CROSS\":\n",
    "            return -1*(e_l-yhat)\n",
    "        if self.lossfunction==\"MSE\":\n",
    "            a=2*(yhat-e_l)\n",
    "            b=np.multiply(yhat,(1-yhat))\n",
    "            ans=(np.multiply(a,b)).astype(float)\n",
    "            return ans\n",
    "            \n",
    "    \n",
    "    #gradient of activation functions\n",
    "    def cal_activationFunc_grad(self,As):\n",
    "        g_dash=[]\n",
    "        if self.activationfunction==\"SIGMOID\":\n",
    "            for i in As:\n",
    "                g_dash.append( self.sigmoid(i) * (1 - self.sigmoid(i) ) )\n",
    "            return g_dash\n",
    "        if self.activationfunction==\"TANH\":\n",
    "            for i in As:\n",
    "                g_dash.append( 1 - np.tanh(z) ** 2)\n",
    "            return g_dash\n",
    "        \n",
    "    def backwardPropogation(self,i,Hs,As,yhat,y_train):\n",
    "        W=self.W\n",
    "        L=self.noOfHL+1\n",
    "        weights_grad=[0]*(L+1)\n",
    "        biases_grad=[0]*(L+1)\n",
    "        activation_grad=[0]*(L+1)\n",
    "        preactivation_grad=[0]*(L+1)\n",
    "        activationFunc_grad=[]\n",
    "        preactivation_grad[L]=self.derivative_wrt_lossFunc(yhat,y_train,i)\n",
    "        for k in range(L+1)[::-1]:\n",
    "            if(i==0):\n",
    "                continue\n",
    "            #gradient of loss wrt to weights at layer k\n",
    "            weights_grad[k]=np.outer(preactivation_grad[k],np.transpose(Hs[k-1]))\n",
    "#             print(\"weights_grad wrt loss at level {} is {}\".format(k,type(weights_grad[k])))\n",
    "            \n",
    "            #gradient of loss wrt to biases at layer k\n",
    "            biases_grad[k]=preactivation_grad[k]\n",
    "#             print(\"baises_grad wrt loss at level {} is {}\".format(k,biases_grad[k].shape))\n",
    "            \n",
    "            #for the next layer calculating gradient of loss wrt to activation\n",
    "            activation_grad[k-1]=np.dot(np.transpose(W[k]),preactivation_grad[k])\n",
    "#             print(\"activation_grad wrt loss at level {} is {}\".format(k,activation_grad[k-1].shape))\n",
    "            \n",
    "            #calculate gradient of activation function wrt preactivation of previous layer\n",
    "            if(k>1):\n",
    "                activationFunc_grad=self.cal_activationFunc_grad(As[k-1])\n",
    "\n",
    "                #for the next layer calculating gradient of loss wrt to preactivation\n",
    "                preactivation_grad[k-1]=np.multiply(activation_grad[k-1],activationFunc_grad)\n",
    "            \n",
    "        return weights_grad,biases_grad\n",
    "    \n",
    "    def acc_grad(self,final_grad,f_g):\n",
    "        L=self.noOfHL+1\n",
    "        for i in range(L+1):\n",
    "            if(i==0):\n",
    "                continue\n",
    "            final_grad[i]=final_grad[i]+f_g[i]\n",
    "        return final_grad\n",
    "    #update the parameters according to \n",
    "    def updateWeights(self,eta,weights_grad):\n",
    "        L=self.noOfHL+1\n",
    "        for i in range(L+1):\n",
    "            if(i==0):\n",
    "                continue\n",
    "            self.W[i]=self.W[i]-(eta)*weights_grad[i]\n",
    "        return\n",
    "    \n",
    "    def updateBiases(self,eta,biases_grad):\n",
    "        L=self.noOfHL+1\n",
    "        for i in range(L+1):\n",
    "            if(i==0):\n",
    "                continue\n",
    "            self.b[i]=self.b[i]-(eta)*biases_grad[i]\n",
    "        return\n",
    "    \n",
    "    #Loss Functions\n",
    "    \n",
    "    def crossEntropy(self,yhat,i):\n",
    "        return -1*np.log(yhat[self.y_train[i]])\n",
    "    \n",
    "    def meanSquaredError(self,yhat,i):\n",
    "        MSE=0\n",
    "        for j in range(self.noOfClass):\n",
    "            if j==self.y_train[i]:\n",
    "                MSE+=(yhat[j]-1)**2\n",
    "            else:\n",
    "                MSE+=(yhat[j])**2\n",
    "        return MSE/self.noOfClass\n",
    "    \n",
    "    def calculateLoss(self,yhat,i):\n",
    "        if self.lossfunction==\"MSE\":\n",
    "            return self.meanSquaredError(yhat,i)\n",
    "        if self.lossfunction==\"CROSS\":\n",
    "            return self.crossEntropy(yhat,i)\n",
    "    \n",
    "    \n",
    "    def calculatePredClasses(self,set_name):\n",
    "        y_pred=[]\n",
    "        if(set_name==\"train\"):\n",
    "            for i in range(self.x_train.shape[0]):\n",
    "                H,A,yhat=self.forwardPropogation(x_train[i])\n",
    "                j=np.argmax(yhat)\n",
    "                y_pred.append(j)\n",
    "        elif(set_name==\"test\"):\n",
    "            for i in range(self.x_test.shape[0]):\n",
    "                H,A,yhat=self.forwardPropogation(x_test[i])\n",
    "                j=np.argmax(yhat)\n",
    "                y_pred.append(j)\n",
    "        elif(set_name==\"validation\"):\n",
    "            for i in range(self.x_valid.shape[0]):\n",
    "                H,A,yhat=self.forwardPropogation(x_valid[i])\n",
    "                j=np.argmax(yhat)\n",
    "                y_pred.append(j)\n",
    "        y_pred=np.array(y_pred)\n",
    "        return y_pred\n",
    "            \n",
    "    def calculateAccuracy(self,set_name):\n",
    "        if(set_name==\"train\"):\n",
    "            y_pred=self.calculatePredClasses(\"train\")\n",
    "            n=y_train.shape[0]\n",
    "        elif(set_name==\"test\"):\n",
    "            y_pred=self.calculatePredClasses(\"test\")\n",
    "            n=y_test.shape[0]\n",
    "        elif(set_name==\"validation\"):\n",
    "            y_pred=self.calculatePredClasses(\"validation\")\n",
    "            n=y_valid.shape[0]\n",
    "        count=0;\n",
    "        if(set_name==\"train\"):\n",
    "            for i in range(y_train.shape[0]):\n",
    "                if y_pred[i]==y_train[i]:\n",
    "                    count+=1\n",
    "        elif(set_name==\"test\"):\n",
    "            for i in range(y_test.shape[0]):\n",
    "                if y_pred[i]==y_test[i]:\n",
    "                    count+=1\n",
    "        elif(set_name==\"validation\"):\n",
    "            for i in range(y_valid.shape[0]):\n",
    "                if y_pred[i]==y_valid[i]:\n",
    "                    count+=1\n",
    "        return ((count/n)*100)\n",
    "\n",
    "        \n",
    "    #Optimizers from here\n",
    "    def _sgd(self):\n",
    "        epochs=self.epochs\n",
    "        L=self.noOfHL+1\n",
    "        k=self.noOfClass\n",
    "        x_train=self.x_train\n",
    "        y_train=self.y_train\n",
    "        eta=self.learningRate\n",
    "        batchSize=self.batchSize\n",
    "        deltaw=[]\n",
    "        deltab=[]\n",
    "        loss=[]\n",
    "        trainingLoss=[]\n",
    "        trainingaccuracy=[]\n",
    "        validationaccuracy=[]\n",
    "        for epoch in range(epochs):\n",
    "            loss=[]\n",
    "            for i in range(x_train.shape[0]):\n",
    "                if(i%batchSize==0):\n",
    "                    if(i!=0):\n",
    "                        #update the weights and biases\n",
    "#                         self.updateWeights(eta,deltaw)\n",
    "                        self.W[1:] = [self.W[i] - eta * deltaw[i] for i in range(1, L+1)]\n",
    "#                         self.updateBiases(eta,deltab)\n",
    "                        self.b[1:] = [self.b[i] - eta * deltab[i] for i in range(1, L+1)]\n",
    "                    Hs,As,yhat=self.forwardPropogation(x_train[i])\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "#                     if(i==0):\n",
    "#                         deltaw=w_g\n",
    "#                         deltab=b_g\n",
    "                    deltaw=w_g\n",
    "                    deltab=b_g\n",
    "#                     else:\n",
    "#                         deltaw=self.acc_grad(deltaw,w_g)\n",
    "#                         deltab=self.acc_grad(deltab,b_g)\n",
    "                else:\n",
    "                    Hs,As,yhat=self.forwardPropogation(x_train[i])\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    deltaw=self.acc_grad(deltaw,w_g)\n",
    "                    deltab=self.acc_grad(deltab,b_g)\n",
    "                    \n",
    "                #append loss for this datapoint\n",
    "                loss.append(self.calculateLoss(yhat,i))\n",
    "\n",
    "                        #update the weights and biases\n",
    "#                   self.updateWeights(eta,deltaw)\n",
    "            self.W[1:] = [self.W[i] - eta * deltaw[i] for i in range(1, L+1)]\n",
    "#                   self.updateBiases(eta,deltab)\n",
    "            self.b[1:] = [self.b[i] - eta * deltab[i] for i in range(1, L+1)]\n",
    "            trainingLoss.append(np.mean(loss))\n",
    "            accuracytrain=self.calculateAccuracy(\"train\")\n",
    "            trainingaccuracy.append(accuracytrain)\n",
    "            accuracyvalid=self.calculateAccuracy(\"validation\")\n",
    "            validationaccuracy.append(accuracyvalid)\n",
    "            print(\"At epoch:{} loss: {} Training accuracy: {} validation accuracy: {}\".format(epoch,trainingLoss[epoch],accuracytrain,accuracyvalid))\n",
    "#             plt.bar(range(len(predClasses)),predClasses)\n",
    "#             plt.title('No of data per class')\n",
    "#             plt.xlabel('Class')\n",
    "#             plt.ylabel('count')\n",
    "#             plt.show()\n",
    "        accuracytrain=self.calculateAccuracy(\"train\")\n",
    "        accuracytest=self.calculateAccuracy(\"test\")\n",
    "        return trainingLoss,accuracytrain,accuracytest\n",
    "    def _mgd(self):\n",
    "        Gamma=self.gamma\n",
    "        epochs=self.epochs\n",
    "        L=self.noOfHL+1\n",
    "        k=self.noOfClass\n",
    "        x_train=self.x_train\n",
    "        y_train=self.y_train\n",
    "        eta=self.learningRate\n",
    "        batchSize=self.batchSize\n",
    "        deltaw=[]\n",
    "        deltab=[]\n",
    "        prev_w=[0]*(L+1)\n",
    "        prev_b=[0]*(L+1)\n",
    "        loss=[]\n",
    "        trainingLoss=[]\n",
    "        trainingaccuracy=[]\n",
    "        validationaccuracy=[]\n",
    "        t=0\n",
    "        for epoch in range(epochs):\n",
    "            loss=[]\n",
    "            for i in range(x_train.shape[0]):\n",
    "                if(i%batchSize==0):\n",
    "                    if(i!=0):\n",
    "                        #update the weights and biases\n",
    "                        if t==0:\n",
    "                            self.W[1:] = [self.W[i] - eta * deltaw[i]/batchSize for i in range(1, L+1)]\n",
    "                            self.b[1:] = [self.b[i] - eta * deltab[i]/batchSize for i in range(1, L+1)]\n",
    "                            prev_w[1:]=[eta*deltaw[i]/batchSize for i in range(1, L+1)]\n",
    "                            prev_b[1:]=[eta*deltab[i]/batchSize for i in range(1, L+1)]\n",
    "                            t=1\n",
    "                        else:\n",
    "                            self.W[1:] = [self.W[i] -  Gamma * prev_w[i] - eta * deltaw[i]/batchSize for i in range(1, L+1)]\n",
    "                            self.b[1:] = [self.b[i] -  Gamma * prev_b[i] - eta * deltab[i]/batchSize for i in range(1, L+1)]\n",
    "                            prev_w[1:]=[Gamma * prev_w[i] + eta*deltaw[i]/batchSize for i in range(1, L+1)]\n",
    "                            prev_b[1:]=[Gamma * prev_b[i] + eta*deltab[i]/batchSize for i in range(1, L+1)]\n",
    "                            \n",
    "                    Hs,As,yhat=self.forwardPropogation(x_train[i])\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    deltaw=w_g\n",
    "                    deltab=b_g\n",
    "#                     else:\n",
    "#                         deltaw=self.acc_grad(deltaw,w_g)\n",
    "#                         deltab=self.acc_grad(deltab,b_g)\n",
    "                else:\n",
    "                    Hs,As,yhat=self.forwardPropogation(x_train[i])\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    deltaw=self.acc_grad(deltaw,w_g)\n",
    "                    deltab=self.acc_grad(deltab,b_g)\n",
    "                    \n",
    "                #append loss for this datapoint\n",
    "                loss.append(self.calculateLoss(yhat,i))\n",
    "\n",
    "            self.W[1:] = [self.W[i] - Gamma * prev_w[i] - eta * deltaw[i]/batchSize for i in range(1, L+1)]\n",
    "            self.b[1:] = [self.b[i] - Gamma * prev_b[i] - eta * deltab[i]/batchSize for i in range(1, L+1)]\n",
    "            \n",
    "            trainingLoss.append(np.mean(loss))\n",
    "            accuracytrain=self.calculateAccuracy(\"train\")\n",
    "            trainingaccuracy.append(accuracytrain)\n",
    "            accuracyvalid=self.calculateAccuracy(\"validation\")\n",
    "            validationaccuracy.append(accuracyvalid)\n",
    "            print(\"At epoch:{} loss: {} Training accuracy: {} validation accuracy: {}\".format(epoch,trainingLoss[epoch],accuracytrain,accuracyvalid))\n",
    "            \n",
    "        accuracytrain=self.calculateAccuracy(\"train\")\n",
    "        accuracytest=self.calculateAccuracy(\"test\")\n",
    "        return trainingLoss,accuracytrain,accuracytest\n",
    "        \n",
    "    def _nag(self):\n",
    "        Gamma=self.gamma\n",
    "        epochs=self.epochs\n",
    "        L=self.noOfHL+1\n",
    "        k=self.noOfClass\n",
    "        x_train=self.x_train\n",
    "        y_train=self.y_train\n",
    "        eta=self.learningRate\n",
    "        batchSize=self.batchSize\n",
    "        deltaw=[]\n",
    "        deltab=[]\n",
    "        prev_w=[0]*(L+1)\n",
    "        prev_b=[0]*(L+1)\n",
    "        v_w=[0]*(L+1)\n",
    "        v_b=[0]*(L+1)\n",
    "        prev_w[1:]=[self.W[i] - self.W[i] for i in range(1, L+1)]\n",
    "        prev_b[1:]=[self.b[i] - self.b[i] for i in range(1, L+1)]\n",
    "        W_l=self.W\n",
    "        b_l=self.b\n",
    "        trainingLoss=[]\n",
    "        trainingaccuracy=[]\n",
    "        validationaccuracy=[]\n",
    "        t=0\n",
    "        for epoch in range(epochs):\n",
    "            loss=[]\n",
    "            for i in range(x_train.shape[0]):\n",
    "                if i!=0:\n",
    "                    self.W=temp_w\n",
    "                    self.b=temp_b\n",
    "                if i%batchSize==0:\n",
    "                    if t==0:\n",
    "                        self.W[1:] = [self.W[i] - Gamma * prev_w[i] for i in range(1, L+1)]\n",
    "                        self.b[1:] = [self.b[i] - Gamma * prev_b[i] for i in range(1, L+1)]\n",
    "                        t=1\n",
    "                    else:\n",
    "                        #Update real W and bs\n",
    "                        v_w[1:] = [Gamma * prev_w[i] + eta * deltaw[i]/batchSize for i in range(1, L+1)]\n",
    "                        v_b[1:] = [Gamma * prev_b[i] + eta * deltab[i]/batchSize for i in range(1, L+1)]\n",
    "                        W_l[1:] = [W_l[i] - v_w[i] for i in range(1, L+1)]\n",
    "                        b_l[1:] = [b_l[i] - v_b[i] for i in range(1, L+1)]\n",
    "                        prev_w=v_w\n",
    "                        prev_b=v_b\n",
    "                        \n",
    "                        #update params for lookahead\n",
    "#                         v_w[1:] = [Gamma * prev_w[i] for i in range(1, L+1)]\n",
    "#                         v_b[1:] = [Gamma * prev_b[i] for i in range(1, L+1)]\n",
    "                        self.W[1:] = [self.W[i] - v_w[i] for i in range(1, L+1)]\n",
    "                        self.b[1:] = [self.b[i] - v_b[i] for i in range(1, L+1)]\n",
    "                        \n",
    "                    Hs,As,yhat=self.forwardPropogation(x_train[i])\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    deltaw=w_g\n",
    "                    deltab=b_g\n",
    "                        \n",
    "                        \n",
    "                else:\n",
    "#                     v_w[1:] = [Gamma * prev_w[i] for i in range(1, L+1)]\n",
    "#                     v_b[1:] = [Gamma * prev_b[i] for i in range(1, L+1)]\n",
    "#                     self.W[1:] = [self.W[i] - v_w[i] for i in range(1, L+1)]\n",
    "#                     self.b[1:] = [self.b[i] - v_b[i] for i in range(1, L+1)]\n",
    "                    Hs,As,yhat=self.forwardPropogation(x_train[i])\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    deltaw=self.acc_grad(deltaw,w_g)\n",
    "                    deltab=self.acc_grad(deltab,b_g)\n",
    "                temp_w=W_l\n",
    "                temp_b=b_l\n",
    "                self.W=W_l\n",
    "                self.b=b_l\n",
    "                \n",
    "                loss.append(self.calculateLoss(yhat,i))\n",
    "            v_w[1:] = [Gamma * prev_w[i] + eta * deltaw[i]/batchSize for i in range(1, L+1)]\n",
    "            v_b[1:] = [Gamma * prev_b[i] + eta * deltab[i]/batchSize for i in range(1, L+1)]\n",
    "            W_l[1:] = [W_l[i] - v_w[i] for i in range(1, L+1)]\n",
    "            b_l[1:] = [b_l[i] - v_b[i] for i in range(1, L+1)]\n",
    "\n",
    "            trainingLoss.append(np.mean(loss))\n",
    "            accuracytrain=self.calculateAccuracy(\"train\")\n",
    "            trainingaccuracy.append(accuracytrain)\n",
    "            accuracyvalid=self.calculateAccuracy(\"validation\")\n",
    "            validationaccuracy.append(accuracyvalid)\n",
    "            print(\"At epoch:{} loss: {} Training accuracy: {} validation accuracy: {}\".format(epoch,trainingLoss[epoch],accuracytrain,accuracyvalid))\n",
    "            \n",
    "                        \n",
    "        accuracytrain=self.calculateAccuracy(\"train\")\n",
    "        accuracytest=self.calculateAccuracy(\"test\")\n",
    "        return trainingLoss,accuracytrain,accuracytest\n",
    "    \n",
    "    def _rmsProp(self):\n",
    "        beta=self.beta\n",
    "        L=self.noOfHL+1\n",
    "        k=self.noOfClass\n",
    "        x_train=self.x_train\n",
    "        y_train=self.y_train\n",
    "        eta=self.learningRate\n",
    "        batchSize=self.batchSize\n",
    "        v_w=[0]*(L+1)\n",
    "        v_b=[0]*(L+1)\n",
    "        deltaw=[]\n",
    "        deltab=[]\n",
    "        prev_w=[0]*(L+1)\n",
    "        prev_b=[0]*(L+1)\n",
    "        loss=[]\n",
    "        trainingLoss=[]\n",
    "        trainingaccuracy=[]\n",
    "        validationaccuracy=[]\n",
    "        t=0\n",
    "        for epoch in range(self.epochs):\n",
    "            loss=[]\n",
    "            for i in range(x_train.shape[0]):\n",
    "                if(i%batchSize==0):\n",
    "                    if(i!=0):\n",
    "                        #update the weights and biases\n",
    "                        if t==0:\n",
    "                            v_w[1:]=[(1-beta) * deltaw[i]**2 for i in range(1, L+1)]\n",
    "                            v_b[1:]=[(1-beta) * deltab[i]**2 for i in range(1, L+1)]\n",
    "                            t=1\n",
    "                        else:\n",
    "                            v_w[1:]=[beta* prev_w[i] + (1-beta) * deltaw[i]**2 for i in range(1, L+1)]\n",
    "                            v_b[1:]=[beta* prev_b[i] + (1-beta) * deltab[i]**2 for i in range(1, L+1)]\n",
    "                            \n",
    "                        self.W[1:] = [self.W[i] - (eta * deltaw[i]) / batchSize*(v_w[i]+epsilon)**0.5 for i in range(1, L+1)]\n",
    "                        self.b[1:] = [self.b[i] - (eta * deltab[i]) / batchSize*(v_b[i]+epsilon)**0.5 for i in range(1, L+1)]\n",
    "                        prev_w=v_w\n",
    "                        prev_b=v_b\n",
    "                            \n",
    "                            \n",
    "                    Hs,As,yhat=self.forwardPropogation(x_train[i])\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    deltaw=w_g\n",
    "                    deltab=b_g\n",
    "                    \n",
    "                else:\n",
    "                    Hs,As,yhat=self.forwardPropogation(x_train[i])\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    deltaw=self.acc_grad(deltaw,w_g)\n",
    "                    deltab=self.acc_grad(deltab,b_g)\n",
    "                    \n",
    "                #append loss for this datapoint\n",
    "                loss.append(self.calculateLoss(yhat,i))\n",
    "\n",
    "            self.W[1:] = [self.W[i] - (eta * deltaw[i]) / batchSize*(v_w[i]+epsilon)**0.5 for i in range(1, L+1)]\n",
    "            self.b[1:] = [self.b[i] - (eta * deltab[i]) / batchSize*(v_b[i]+epsilon)**0.5 for i in range(1, L+1)]\n",
    "            \n",
    "            trainingLoss.append(np.mean(loss))\n",
    "            accuracytrain=self.calculateAccuracy(\"train\")\n",
    "            trainingaccuracy.append(accuracytrain)\n",
    "            accuracyvalid=self.calculateAccuracy(\"validation\")\n",
    "            validationaccuracy.append(accuracyvalid)\n",
    "            print(\"At epoch:{} loss: {} Training accuracy: {} validation accuracy: {}\".format(epoch,trainingLoss[epoch],accuracytrain,accuracyvalid))\n",
    "            \n",
    "        accuracytrain=self.calculateAccuracy(\"train\")\n",
    "        accuracytest=self.calculateAccuracy(\"test\")\n",
    "        return trainingLoss,accuracytrain,accuracytest\n",
    "    \n",
    "    def _adam(self):\n",
    "        Beta1=self.Beta1;\n",
    "        Beta2=self.Beta2;\n",
    "        epochs=self.epochs\n",
    "        L=self.noOfHL+1\n",
    "        k=self.noOfClass\n",
    "        x_train=self.x_train\n",
    "        y_train=self.y_train\n",
    "        eta=self.learningRate\n",
    "        batchSize=self.batchSize\n",
    "        deltaw=[]\n",
    "        deltab=[]\n",
    "        v_w=[0]*(L+1)\n",
    "        v_b=[0]*(L+1)\n",
    "        m_w=[0]*(L+1)\n",
    "        m_b=[0]*(L+1)\n",
    "        v_w_hat=[0]*(L+1)\n",
    "        v_b_hat=[0]*(L+1)\n",
    "        m_w_hat=[0]*(L+1)\n",
    "        m_b_hat=[0]*(L+1)\n",
    "        loss=[]\n",
    "        trainingLoss=[]\n",
    "        trainingaccuracy=[]\n",
    "        validationaccuracy=[]\n",
    "        t=1\n",
    "        for epoch in range(epochs):\n",
    "            loss=[]\n",
    "            for i in range(x_train.shape[0]):\n",
    "                if(i%batchSize==0):\n",
    "                    if(i!=0):\n",
    "                        #update the weights and biases\n",
    "                        if t==1:\n",
    "                            m_w[1:] = [(1-Beta1)* deltaw[i]/batchSize for i in range(1, L+1)]\n",
    "                            m_b[1:] = [(1-Beta1)* deltab[i]/batchSize for i in range(1, L+1)]\n",
    "                            v_w[1:]=[(1-Beta2)*np.multiply(deltaw[i]/batchSize,deltaw[i]/batchSize) for i in range(1, L+1)]\n",
    "                            v_b[1:]=[(1-Beta2)*np.multiply(deltab[i]/batchSize,deltab[i]/batchSize) for i in range(1, L+1)]\n",
    "                            \n",
    "                            m_w_hat[1:]=[m_w[i]/(1-np.power(Beta1,t)) for i in range(1, L+1)]\n",
    "                            m_b_hat[1:]=[m_b[i]/(1-np.power(Beta1,t)) for i in range(1, L+1)]\n",
    "                            v_w_hat[1:]=[v_w[i]/(1-np.power(Beta2,t)) for i in range(1, L+1)]\n",
    "                            v_b_hat[1:]=[v_b[i]/(1-np.power(Beta2,t)) for i in range(1, L+1)]\n",
    "                            \n",
    "                            self.W[1:] = [self.W[i] -  eta * m_w_hat[i]/(np.sqrt(v_w_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "                            self.b[1:] = [self.b[i] -  eta * m_b_hat[i]/(np.sqrt(v_b_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "                            t+=1\n",
    "                        else:\n",
    "                            m_w[1:] = [Beta1*m_w[i]+(1-Beta1)* deltaw[i]/batchSize for i in range(1, L+1)]\n",
    "                            m_b[1:] = [Beta1*m_b[i]+(1-Beta1)* deltab[i]/batchSize for i in range(1, L+1)]\n",
    "                            v_w[1:] = [Beta2*v_w[i]+(1-Beta2)*np.multiply(deltaw[i]/batchSize,deltaw[i]/batchSize) for i in range(1, L+1)]\n",
    "                            v_b[1:] = [Beta2*v_b[i]+(1-Beta2)*np.multiply(deltab[i]/batchSize,deltab[i]/batchSize) for i in range(1, L+1)]\n",
    "                            \n",
    "                            m_w_hat[1:]=[m_w[i]/(1-np.power(Beta1,t)) for i in range(1, L+1)]\n",
    "                            m_b_hat[1:]=[m_b[i]/(1-np.power(Beta1,t)) for i in range(1, L+1)] \n",
    "                            v_w_hat[1:]=[v_w[i]/(1-np.power(Beta2,t)) for i in range(1, L+1)]\n",
    "                            v_b_hat[1:]=[v_b[i]/(1-np.power(Beta2,t)) for i in range(1, L+1)]\n",
    "                            \n",
    "                            self.W[1:] = [self.W[i] -  eta * m_w_hat[i]/(np.sqrt(v_w_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "                            self.b[1:] = [self.b[i] -  eta * m_b_hat[i]/(np.sqrt(v_b_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "                            t+=1\n",
    "                            \n",
    "                    Hs,As,yhat=self.forwardPropogation(x_train[i])\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    deltaw=w_g\n",
    "                    deltab=b_g\n",
    "                else:\n",
    "                    Hs,As,yhat=self.forwardPropogation(x_train[i])\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    deltaw=self.acc_grad(deltaw,w_g)\n",
    "                    deltab=self.acc_grad(deltab,b_g)\n",
    "                    \n",
    "                #append loss for this datapoint\n",
    "                loss.append(self.calculateLoss(yhat,i))\n",
    "\n",
    "            self.W[1:] = [self.W[i] -  eta * m_w_hat[i]/(np.sqrt(v_w_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "            self.b[1:] = [self.b[i] -  eta * m_b_hat[i]/(np.sqrt(v_b_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "            \n",
    "            trainingLoss.append(np.mean(loss))\n",
    "            accuracytrain=self.calculateAccuracy(\"train\")\n",
    "            trainingaccuracy.append(accuracytrain)\n",
    "            accuracyvalid=self.calculateAccuracy(\"validation\")\n",
    "            validationaccuracy.append(accuracyvalid)\n",
    "            print(\"At epoch:{} loss: {} Training accuracy: {} validation accuracy: {}\".format(epoch,trainingLoss[epoch],accuracytrain,accuracyvalid))\n",
    "            \n",
    "        accuracytrain=self.calculateAccuracy(\"train\")\n",
    "        accuracytest=self.calculateAccuracy(\"test\")\n",
    "        return trainingLoss,accuracytrain,accuracytest\n",
    "        \n",
    "    def _nadam(self):\n",
    "        Beta1=self.Beta1;\n",
    "        Beta2=self.Beta2;\n",
    "        epochs=self.epochs\n",
    "        L=self.noOfHL+1\n",
    "        k=self.noOfClass\n",
    "        x_train=self.x_train\n",
    "        y_train=self.y_train\n",
    "        eta=self.learningRate\n",
    "        batchSize=self.batchSize\n",
    "        deltaw=[]\n",
    "        deltab=[]\n",
    "        v_w=[0]*(L+1)\n",
    "        v_b=[0]*(L+1)\n",
    "        m_w=[0]*(L+1)\n",
    "        m_b=[0]*(L+1)\n",
    "        v_w_hat=[0]*(L+1)\n",
    "        v_b_hat=[0]*(L+1)\n",
    "        m_w_hat=[0]*(L+1)\n",
    "        m_b_hat=[0]*(L+1)\n",
    "        W_l=self.W\n",
    "        b_l=self.b\n",
    "        loss=[]\n",
    "        trainingLoss=[]\n",
    "        trainingaccuracy=[]\n",
    "        validationaccuracy=[]\n",
    "        t=0\n",
    "        for epoch in range(epochs):\n",
    "            loss=[]\n",
    "            for i in range(x_train.shape[0]):\n",
    "                if i!=0:\n",
    "                    self.W=temp_w\n",
    "                    self.b=temp_b\n",
    "                if(i%batchSize==0):\n",
    "                    if(i!=0):\n",
    "                        #update the weights and biases\n",
    "                        if t==0:\n",
    "                            m_w[1:] = [(1-Beta1)* deltaw[i]/batchSize for i in range(1, L+1)]\n",
    "                            m_b[1:] = [(1-Beta1)* deltab[i]/batchSize for i in range(1, L+1)]\n",
    "                            v_w[1:]=[(1-Beta2)*np.multiply(deltaw[i]/batchSize,deltaw[i]/batchSize) for i in range(1, L+1)]\n",
    "                            v_b[1:]=[(1-Beta2)*np.multiply(deltab[i]/batchSize,deltab[i]/batchSize) for i in range(1, L+1)]\n",
    "                            \n",
    "                            m_w_hat[1:]=[m_w[i]/(1-np.power(Beta1,1)) for i in range(1, L+1)]\n",
    "                            m_b_hat[1:]=[m_b[i]/(1-np.power(Beta1,1)) for i in range(1, L+1)]\n",
    "                            v_w_hat[1:]=[v_w[i]/(1-np.power(Beta2,1)) for i in range(1, L+1)]\n",
    "                            v_b_hat[1:]=[v_b[i]/(1-np.power(Beta2,1)) for i in range(1, L+1)]\n",
    "                            \n",
    "                            self.W[1:] = [self.W[i] -  eta * m_w_hat[i]/(np.sqrt(v_w_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "                            self.b[1:] = [self.b[i] -  eta * m_b_hat[i]/(np.sqrt(v_b_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "                            t+=1\n",
    "                        else:\n",
    "                            m_w[1:] = [Beta1*m_w[i]+(1-Beta1)* deltaw[i]/batchSize for i in range(1, L+1)]\n",
    "                            m_b[1:] = [Beta1*m_b[i]+(1-Beta1)* deltab[i]/batchSize for i in range(1, L+1)]\n",
    "                            v_w[1:] = [Beta2*v_w[i]+(1-Beta2)*np.multiply(deltaw[i]/batchSize,deltaw[i]/batchSize) for i in range(1, L+1)]\n",
    "                            v_b[1:] = [Beta2*v_b[i]+(1-Beta2)*np.multiply(deltab[i]/batchSize,deltab[i]/batchSize) for i in range(1, L+1)]\n",
    "                            \n",
    "                            m_w_hat[1:]=[m_w[i]/(1-np.power(Beta1,t)) for i in range(1, L+1)]\n",
    "                            m_b_hat[1:]=[m_b[i]/(1-np.power(Beta1,t)) for i in range(1, L+1)] \n",
    "                            v_w_hat[1:]=[v_w[i]/(1-np.power(Beta2,t)) for i in range(1, L+1)]\n",
    "                            v_b_hat[1:]=[v_b[i]/(1-np.power(Beta2,t)) for i in range(1, L+1)]\n",
    "                            \n",
    "                            W_l[1:] = [W_l[i] -  eta * m_w_hat[i]/(np.sqrt(v_w_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "                            b_l[1:] = [b_l[i] -  eta * m_b_hat[i]/(np.sqrt(v_b_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "                            #update the lookaheads\n",
    "                            self.W[1:] = [self.W[i] -  eta * m_w_hat[i]/(np.sqrt(v_w_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "                            self.b[1:] = [self.b[i] -  eta * m_b_hat[i]/(np.sqrt(v_b_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "                            t+=1\n",
    "                            \n",
    "                    Hs,As,yhat=self.forwardPropogation(x_train[i])\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    deltaw=w_g\n",
    "                    deltab=b_g\n",
    "                else:\n",
    "                    Hs,As,yhat=self.forwardPropogation(x_train[i])\n",
    "                    w_g,b_g=self.backwardPropogation(i,Hs,As,yhat,y_train)\n",
    "                    deltaw=self.acc_grad(deltaw,w_g)\n",
    "                    deltab=self.acc_grad(deltab,b_g)\n",
    "                temp_w=W_l\n",
    "                temp_b=b_l\n",
    "                self.W=W_l\n",
    "                self.b=b_l\n",
    "                    \n",
    "                #append loss for this datapoint\n",
    "                loss.append(self.calculateLoss(yhat,i))\n",
    "\n",
    "            self.W[1:] = [self.W[i] -  eta * m_w_hat[i]/(np.sqrt(v_w_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "            self.b[1:] = [self.b[i] -  eta * m_b_hat[i]/(np.sqrt(v_b_hat[i]+epsilon)) for i in range(1, L+1)]\n",
    "            \n",
    "            trainingLoss.append(np.mean(loss))\n",
    "            accuracytrain=self.calculateAccuracy(\"train\")\n",
    "            trainingaccuracy.append(accuracytrain)\n",
    "            accuracyvalid=self.calculateAccuracy(\"validation\")\n",
    "            validationaccuracy.append(accuracyvalid)\n",
    "            print(\"At epoch:{} loss: {} Training accuracy: {} validation accuracy: {}\".format(epoch,trainingLoss[epoch],accuracytrain,accuracyvalid))\n",
    "            \n",
    "        accuracytrain=self.calculateAccuracy(\"train\")\n",
    "        accuracytest=self.calculateAccuracy(\"test\")\n",
    "        return trainingLoss,accuracytrain,accuracytest\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "               \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "At epoch:0 loss: 0.802897524913027 Training accuracy: 83.03333333333333 validation accuracy: 82.63333333333334\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "#importing the dataset from keras library\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# #one data image per label\n",
    "# fig,ax=plt.subplots(nrows=10,figsize=(15,15))\n",
    "# for i in range(10):\n",
    "#     ax[i].set_title(\"\\n class {} image\".format(i))\n",
    "#     ax[i].axis(\"off\")\n",
    "#     x=x_train[y_train==i]\n",
    "#     ax[i].imshow(x[0,:,:],cmap=\"gray\")\n",
    "\n",
    "#normalizing the data between 0-1\n",
    "x_train=x_train/255\n",
    "x_train=x_train.astype(float);\n",
    "x_test=x_test/255\n",
    "x_test=x_test.astype(float);\n",
    "\n",
    "#flattening the data points to 1D\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "\n",
    "#creating validation set\n",
    "num_valid = int(0.1 * x_train.shape[0])  #using 10% of the data as validation test for the model and remaining for train\n",
    "x_valid = x_train[:num_valid, :] \n",
    "y_valid = y_train[:num_valid] \n",
    "x_train = x_train[num_valid:, :] \n",
    "y_train = y_train[num_valid:]  \n",
    "\n",
    "NeuronsPL=32\n",
    "epochs=10\n",
    "noOfHL=2\n",
    "lossfunction=\"CROSS\"\n",
    "activationFunc=\"SIGMOID\"\n",
    "learningRate=0.001\n",
    "batchSize=32\n",
    "optimizer=\"NADAM\"\n",
    "gamma=0.8\n",
    "initialize=\"XAVIER\"\n",
    "Beta=0.7\n",
    "Beta1=0.9\n",
    "Beta2=0.999\n",
    "epsilon=0.00001\n",
    "\n",
    "#creating the object\n",
    "FWNN=FeedForwardNN(epochs,noOfHL,NeuronsPL,10,x_train,y_train,x_valid,y_valid,x_test,y_test,optimizer,activationFunc,learningRate,batchSize,initialize,lossfunction,gamma,Beta,Beta1,Beta2,epsilon)\n",
    "\n",
    "#predicting before training the model\n",
    "y_pred=FWNN.calculatePredClasses(\"train\")\n",
    "print(y_pred)\n",
    "\n",
    "#training the model using sgd\n",
    "loss,accuracytrain,accuracytest=FWNN.optimizer()\n",
    "print(\"Accuracy of this model after {} epochs is training accuracy:{} and test accuracy:{}\".format(epochs,accuracytrain,accuracytest))\n",
    "\n",
    "y_pred=FWNN.calculatePredClasses(\"train\")\n",
    "print(y_pred)\n",
    "print(y_train)\n",
    "\n",
    "#printing confusion matrix for visualization\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# y_pred = np.array([np.argmax(FNNN.forwardPropogation(i)[2]) for i in range(len(x_train))])\n",
    "sns.heatmap(confusion_matrix(y_pred.reshape(x_train.shape[0],),y_train.reshape(x_train.shape[0],)),annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_pred)\n",
    "sns.distplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22953184",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=FWNN.calculatePredClasses(\"test\")\n",
    "sns.heatmap(confusion_matrix(y_pred,y_test),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b910e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(loss)),loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49f6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
